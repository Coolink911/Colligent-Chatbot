# ğŸ¤– Collins' Personal AI Assistant

A context-aware chatbot powered by **RAG (Retrieval-Augmented Generation)** that answers questions based on personal documents and research. Collins' Personal AI Assistant provides intelligent responses grounded in specific content through advanced natural language processing and vector search.

## ğŸš€ Quick Start

### **Streamlit Web Interface**
```bash
# Install dependencies
pip install -r requirements.txt

# Run the web app
streamlit run colligent_web_app.py
```
Access at: `http://localhost:8501`

## ğŸ“ Project Structure

```
collins_personal_agent/
â”œâ”€â”€ ğŸ“„ Core Application
â”‚   â”œâ”€â”€ colligent_web_app.py          # Main Streamlit interface
â”‚   â”œâ”€â”€ colligent_core.py             # Core chatbot logic
â”‚   â”œâ”€â”€ colligent_config.py           # Configuration settings
â”‚   â”œâ”€â”€ colligent_vector_db.py        # Vector database operations
â”‚   â””â”€â”€ colligent_document_processor.py # Document processing
â”‚
â”œâ”€â”€ ğŸ“š Knowledge Base
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ Collins_cv_2025-1-2.pdf   # CV document
â”‚       â”œâ”€â”€ Collins_cv_2025-1-2.txt   # CV text version
â”‚       â”œâ”€â”€ Draft msc.pdf             # Research document
â”‚       â”œâ”€â”€ Draft msc.txt             # Research text version
â”‚       â”œâ”€â”€ best_model.py             # Diffusion model code
â”‚       â””â”€â”€ best_model.txt            # Model documentation
â”‚
â”œâ”€â”€ ğŸ“‹ Configuration
â”‚   â”œâ”€â”€ README.md                     # Project documentation
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                    # Container deployment
â”‚   â”œâ”€â”€ Procfile                      # Heroku deployment
â”‚   â””â”€â”€ runtime.txt                   # Python version
â”‚
â””â”€â”€ ğŸ—„ï¸ Database
    â””â”€â”€ vector_db/                    # ChromaDB vector store
```

## ğŸ¯ Features

### **ğŸ¤– Intelligent Responses**
- **RAG-Powered**: Retrieves relevant information from documents
- **Context-Aware**: Answers based on actual content, not generic knowledge
- **Multi-Modal**: Supports PDF and text documents
- **Fallback System**: Works without external APIs

### **ğŸ­ Response Modes**
- **ğŸ’¬ Default**: Natural conversational tone
- **ğŸ‘” Interview**: Professional & concise
- **ğŸ“– Storytelling**: Narrative & reflective
- **âš¡ Fast Facts**: Bullet points & TL;DR
- **ğŸ’ª Humble Brag**: Confident self-promotion
- **ğŸ’» Code Style**: Technical & implementation-focused

### **ğŸ”§ Interactive Features**
- **Real-time Chat**: Immediate responses with source attribution
- **Context Display**: See which documents were used
- **Mode Switching**: Change response style on the fly
- **Knowledge Base Management**: Rebuild and update documents



## ğŸ’¡ Sample Questions & Responses

### **Professional Background**
**Q: "What kind of engineer am I?"**
```
Expected Answer: "I'm a Data Scientist and Astrophysics Researcher. I work at the intersection of machine learning and cosmology, specializing in predicting universe seeing conditions, emulating cosmic structures with AI, and solving theoretical puzzles in astrophysics."

RAG Process:
1. Retrieval: Finds chunks about role, specialization, and background
2. Generation: Combines information into coherent professional description
3. Sources: Collins_cv_2025-1-2.pdf, Draft msc.pdf
```

### **Technical Expertise**
**Q: "Explain my diffusion model research"**
```
Expected Answer: "My diffusion model research focuses on emulating neutral hydrogen maps using a ContextU-Net architecture. I implement residual convolutional blocks with batch normalization and ReLU activation, supporting both residual and non-residual modes with dynamic channel adjustment."

RAG Process:
1. Retrieval: Identifies technical implementation details from best_model.txt
2. Generation: Synthesizes technical approach with implementation specifics
3. Sources: best_model.txt, best_model.py
```

## ğŸ› ï¸ Technical Architecture

### **Core Components**
1. **Document Processor**: Extracts and chunks text from PDFs and documents
2. **Vector Store**: ChromaDB with sentence transformers for semantic search
3. **Chatbot Engine**: OpenAI GPT + fallback system for response generation
4. **Web Interfaces**: Streamlit and Flask for different use cases

### **RAG Pipeline**
1. **ğŸ“„ Document Loading**: PDF extraction and text processing
2. **âœ‚ï¸ Chunking**: 1000-character chunks with 200-character overlap
3. **ğŸ” Embedding**: Semantic vectorization using all-MiniLM-L6-v2
4. **ğŸ¯ Retrieval**: Top-5 most similar chunks for context
5. **ğŸ¤– Generation**: Context-aware response with source attribution

## ğŸ”§ Configuration

### **Environment Variables**
```bash
# Copy env_example.txt to .env and fill in your values
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo
```

### **Security Features**
- **Input Validation**: Prevents XSS and injection attacks
- **Rate Limiting**: 30 requests per minute per user
- **Output Sanitization**: Removes potentially harmful content
- **Session Management**: Secure session handling
- **CORS Protection**: Disabled for local deployment
- **XSRF Protection**: Enabled for form security

### **Customization**
- Add documents to `data/` folder
- Modify response modes in `colligent_core.py`
- Customize UI in `colligent_web_app.py`
- Adjust RAG parameters in `colligent_config.py`

## ğŸ“Š Knowledge Base

### **Current Documents**
- **CV & Background**: Professional experience and skills
- **Research Papers**: Academic work and publications
- **Code Documentation**: Technical implementations and approaches

### **Adding New Documents**
1. Place files in `data/` directory
2. Restart the application
3. Use "Rebuild KB" button in web interface

## ğŸ¨ Customization

- **Add Documents**: Place files in `data/` directory
- **Modify Response Modes**: Edit `colligent_core.py`
- **Customize UI**: Edit `colligent_web_app.py`
- **Adjust RAG Parameters**: Edit `colligent_config.py`

## ğŸš€ Deployment

### **Streamlit Cloud (Recommended)**
1. Push code to GitHub
2. Connect to Streamlit Cloud
3. Deploy automatically

### **Heroku**
1. Add `Procfile` and `runtime.txt`
2. Configure buildpacks
3. Deploy via Heroku CLI

### **Docker**
```bash
docker build -t colligent .
docker run -p 8501:8501 colligent
```

## ğŸ“„ License

This project is for personal use and educational purposes.

---

**Collins' Personal AI Assistant** | Powered by RAG Technology ğŸ¤–âœ¨
